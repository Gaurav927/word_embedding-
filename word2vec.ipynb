{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    stops = stopwords.words('english')\n",
    "    #print(stops)\n",
    "    porter = PorterStemmer()\n",
    "    for word in sentence.split():\n",
    "        if word in stops:\n",
    "            sentence = sentence.replace(word, '')\n",
    "        sentence = sentence.replace(word, porter.stem(word))\n",
    "    sentence = \" \".join(sentence.split())\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def Map(text):\n",
    "    a = []\n",
    "    for word in text.split(' '):\n",
    "        a.append(word_to_id[word])\n",
    "    return a\n",
    "\n",
    "def prepare_training_data(text):\n",
    "    return list(map(Map,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('SPAM text message 20170820 - Data.csv')\n",
    "text = list(text.Message)\n",
    "text = [preprocess_text(text_) for text_ in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = mapping(\" \".join(text).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = prepare_training_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data N X window_size, label as N X 1\n",
    "\n",
    "def prepare_data(train,window_size=2):\n",
    "    training_data = []\n",
    "    training_label = []\n",
    "    \n",
    "    for data in train:\n",
    "        if(len(data)<window_size):\n",
    "            continue\n",
    "        for i in range(window_size,len(data)-window_size-1):\n",
    "            training_data.append(data[i])\n",
    "            training_label.append(data[i-window_size:i] + data[i+1:i+window_size+1])\n",
    "    return training_data,training_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data,training_label = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(word_index,word_to_id):\n",
    "    num = len(word_to_id)\n",
    "    \n",
    "    vector = np.zeros(shape=(num))\n",
    "    \n",
    "    pos = word_index\n",
    "    \n",
    "    vector[pos] =1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_parameter(word_to_id,latent_dim=5):\n",
    "    \n",
    "#     num = len(word_to_id)\n",
    "#     latent_dim =latent_dim\n",
    "    \n",
    "#     parameter_0 = np.random.normal( size = (num,latent_dim))\n",
    "#     parameter_1 = np.random.normal(size = (latent_dim,num))\n",
    "    \n",
    "#     return parameter_0,parameter_1\n",
    "\n",
    "num,latent_dim = len(word_to_id) , 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(training_data,training_label,batch_size = 16):\n",
    "    \n",
    "    for i in range((len(training_label)//batch_size)-1):\n",
    "        \n",
    "        yield training_data[i*batch_size:(i+1)*batch_size], training_label[i*batch_size:(i+1)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.dot(v1_u, v2_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(latent_dim,input_shape = [num]))\n",
    "model.add(keras.layers.Dense(num,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5)                 36155     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7230)              43380     \n",
      "=================================================================\n",
      "Total params: 79,535\n",
      "Trainable params: 79,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1208 15:36:48.745772 4552582592 deprecation.py:323] From /anaconda3/envs/generative/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 15686.53\n",
      "Epoch 1 loss 15642.09\n",
      "Epoch 2 loss 15597.68\n",
      "Epoch 3 loss 15552.65\n",
      "Epoch 4 loss 15504.56\n",
      "Epoch 5 loss 15444.15\n",
      "Epoch 6 loss 15337.16\n",
      "Epoch 7 loss 15073.32\n",
      "Epoch 8 loss 14542.46\n",
      "Epoch 9 loss 14018.74\n",
      "Epoch 10 loss 13692.73\n",
      "Epoch 11 loss 13513.98\n",
      "Epoch 12 loss 13410.26\n",
      "Epoch 13 loss 13344.11\n",
      "Epoch 14 loss 13298.72\n",
      "Epoch 15 loss 13265.36\n",
      "Epoch 16 loss 13238.96\n",
      "Epoch 17 loss 13216.33\n",
      "Epoch 18 loss 13195.21\n",
      "Epoch 19 loss 13173.88\n",
      "Epoch 20 loss 13151.07\n",
      "Epoch 21 loss 13126.17\n",
      "Epoch 22 loss 13099.69\n",
      "Epoch 23 loss 13073.22\n",
      "Epoch 24 loss 13048.56\n",
      "Epoch 25 loss 13026.84\n",
      "Epoch 26 loss 13008.22\n",
      "Epoch 27 loss 12992.02\n",
      "Epoch 28 loss 12977.32\n",
      "Epoch 29 loss 12963.44\n"
     ]
    }
   ],
   "source": [
    "## start forward network\n",
    "NUM_EPOCH = 30\n",
    "batch_size = 64\n",
    "for i in range(NUM_EPOCH):\n",
    "    \n",
    "    gen = create_batch(training_data,training_label,batch_size)\n",
    "    \n",
    "    Loss = 0\n",
    "\n",
    "    for iterat in range((len(training_label)//batch_size)-1):\n",
    "        data, label = next(gen)\n",
    "        trainX = []\n",
    "        for data_ in label:\n",
    "            data_ = np.sum([one_hot_vector(d,word_to_id) for d in data_],axis=0)\n",
    "            trainX.append(data_)\n",
    "            \n",
    "        trainY = np.array([[one_hot_vector(y,word_to_id)] for y in label])\n",
    "        trainX = np.array(trainX)\n",
    "\n",
    "        trainY = np.squeeze(trainY,axis =1)\n",
    "        loss = model.train_on_batch(trainX,trainY)\n",
    "        Loss += loss[0]\n",
    "    print(\"Epoch {} loss {}\".format(i,np.round(Loss,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array(model.get_weights()[0])\n",
    "W2 = np.array(model.get_weights()[2].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING = (W1 + W2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95900124"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_between(WORD_EMBEDDING[word_to_id['walk']],WORD_EMBEDDING[word_to_id['run']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very closely related "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
